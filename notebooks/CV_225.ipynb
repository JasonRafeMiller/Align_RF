{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG-tGRnlFLA3"
      },
      "source": [
        "# Cross Validation\n",
        "Random Forest.\n",
        "\n",
        "Equus, Star, genomes.\n",
        "\n",
        "Stats from Sorted.bam files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RmwUsVLFLA6",
        "outputId": "e5371001-7484-4a65-b64c-fe0d87af8f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 14:04:18.170916\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlzN9OdsFWEU",
        "outputId": "de200bd0-199d-4572-dcca-f45960d7289d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU device not found\n",
            "Running on CoLab\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    print('Running on CoLab')\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATA_DIR=PATH+'My Drive/data/IRP2/Equus/STAR_DNA/'  # must end in \"/\"\n",
        "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print('Running on Mac')\n",
        "    DATA_DIR=\"Equus/STAR_DNA/\"\n",
        "    MODEL_DIR=\"Models/\"\n",
        "SAVE_MODEL_FILENAME = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIAT2G5DYwvS",
        "outputId": "d6e400ef-6786-4c1d-fb87-b6030f2ae47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "sklearn 1.2.2\n"
          ]
        }
      ],
      "source": [
        "from platform import python_version\n",
        "print('Python',python_version())\n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42) # supposedly sets scikit-learn\n",
        "import pandas as pd  # for plotting\n",
        "import time # sleep function\n",
        "from os.path import isfile\n",
        "import gzip\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn   # pip install --upgrade scikit-learn\n",
        "print('sklearn',sklearn.__version__)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "\n",
        "EPOCHS=150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtqdpJOxFLBA"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnkpVKdMFLA-",
        "outputId": "959a8c0a-7463-429f-b42c-3e5703eb7a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory: /content/drive/My Drive/data/IRP2/Equus/STAR_DNA/\n",
            "Data file 0 filtered.asinus_read_stats.csv.gz\n",
            "Data file 1 filtered.caballus_read_stats.csv.gz\n",
            "Input lines for training: 1000000\n"
          ]
        }
      ],
      "source": [
        "MAX_LINES_TO_LOAD =    1000000 # training - 1M lines requires 2GB RAM\n",
        "#MAX_LINES_TO_LOAD =    10000 # use this for debugging\n",
        "\n",
        "VALID_PORTION = 0.20\n",
        "\n",
        "DATA_FILE_0 = 'filtered.asinus_read_stats.csv.gz'\n",
        "DATA_FILE_1 = 'filtered.caballus_read_stats.csv.gz'\n",
        "\n",
        "print('Data directory: %s'%DATA_DIR)\n",
        "print('Data file 0 %s'%DATA_FILE_0)\n",
        "print('Data file 1 %s'%DATA_FILE_1)\n",
        "print('Input lines for training: %d'%MAX_LINES_TO_LOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUBLdpVEVQ3I",
        "outputId": "12c16173-d246-410d-f5c7-ceb6b777cb22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total feature names:  53\n",
            "0 P1 R1 AS\n",
            "1 P1 R1 ED\n",
            "2 P1 R1 MM\n",
            "3 P1 R1 HQMM\n",
            "4 P1 R1 GO\n",
            "5 P1 R1 GE\n",
            "6 P1 R1 INS\n",
            "7 P1 R1 DELS\n",
            "8 P1 R1 HQINS\n",
            "9 P1 R1 HQDEL\n",
            "10 P1 R2 AS\n",
            "11 P1 R2 ED\n",
            "12 P1 R2 MM\n",
            "13 P1 R2 HQMM\n",
            "14 P1 R2 GO\n",
            "15 P1 R2 GE\n",
            "16 P1 R2 INS\n",
            "17 P1 R2 DELS\n",
            "18 P1 R2 HQINS\n",
            "19 P1 R2 HQDEL\n",
            "20 P2 R1 AS\n",
            "21 P2 R1 ED\n",
            "22 P2 R1 MM\n",
            "23 P2 R1 HQMM\n",
            "24 P2 R1 GO\n",
            "25 P2 R1 GE\n",
            "26 P2 R1 INS\n",
            "27 P2 R1 DELS\n",
            "28 P2 R1 HQINS\n",
            "29 P2 R1 HQDEL\n",
            "30 P2 R2 AS\n",
            "31 P2 R2 ED\n",
            "32 P2 R2 MM\n",
            "33 P2 R2 HQMM\n",
            "34 P2 R2 GO\n",
            "35 P2 R2 GE\n",
            "36 P2 R2 INS\n",
            "37 P2 R2 DELS\n",
            "38 P2 R2 HQINS\n",
            "39 P2 R2 HQDEL\n",
            "40 Span diff\n",
            "41 AS diff\n",
            "42 ED diff\n",
            "43 MAT diff\n",
            "44 MM diff\n",
            "45 HQMM diff\n",
            "46 GO diff\n",
            "47 GE diff\n",
            "48 INS diff\n",
            "49 DELS diff\n",
            "50 HQINS diff\n",
            "51 HQDEL diff\n",
            "52 PARENT\n"
          ]
        }
      ],
      "source": [
        "# P1 parent 1\n",
        "# R1 read 1\n",
        "# PS primary or secondary\n",
        "# AS bowtie alignment score (0 is best)\n",
        "# ED edit distance\n",
        "# MM mismatch count\n",
        "# GO gap open count\n",
        "# GE gap extend count\n",
        "feature_names = [\n",
        "    'P1 R1 AS',\n",
        "    'P1 R1 ED',\n",
        "    ##'P1 R1 MAT',\n",
        "    'P1 R1 MM',\n",
        "    'P1 R1 HQMM',\n",
        "    'P1 R1 GO',\n",
        "    'P1 R1 GE',\n",
        "    'P1 R1 INS',\n",
        "    'P1 R1 DELS',\n",
        "    'P1 R1 HQINS',\n",
        "    'P1 R1 HQDEL',\n",
        "    'P1 R2 AS',\n",
        "    'P1 R2 ED',\n",
        "    ##'P1 R2 MAT',\n",
        "    'P1 R2 MM',\n",
        "    'P1 R2 HQMM',\n",
        "    'P1 R2 GO',\n",
        "    'P1 R2 GE',\n",
        "    'P1 R2 INS',\n",
        "    'P1 R2 DELS',\n",
        "    'P1 R2 HQINS',\n",
        "    'P1 R2 HQDEL',\n",
        "    'P2 R1 AS',\n",
        "    'P2 R1 ED',\n",
        "    ##'P2 R1 MAT',\n",
        "    'P2 R1 MM',\n",
        "    'P2 R1 HQMM',\n",
        "    'P2 R1 GO',\n",
        "    'P2 R1 GE',\n",
        "    'P2 R1 INS',\n",
        "    'P2 R1 DELS',\n",
        "    'P2 R1 HQINS',\n",
        "    'P2 R1 HQDEL',\n",
        "    'P2 R2 AS',\n",
        "    'P2 R2 ED',\n",
        "    ##'P2 R2 MAT',\n",
        "    'P2 R2 MM',\n",
        "    'P2 R2 HQMM',\n",
        "    'P2 R2 GO',\n",
        "    'P2 R2 GE',\n",
        "    'P2 R2 INS',\n",
        "    'P2 R2 DELS',\n",
        "    'P2 R2 HQINS',\n",
        "    'P2 R2 HQDEL',\n",
        "    ##'R1 length',\n",
        "    ##'R2 length',\n",
        "    ##'P1 span',\n",
        "    ##'P2 span',\n",
        "    'Span diff',\n",
        "    'AS diff',\n",
        "    'ED diff',\n",
        "    'MAT diff',\n",
        "    'MM diff',\n",
        "    'HQMM diff',\n",
        "    'GO diff',\n",
        "    'GE diff',\n",
        "    'INS diff',\n",
        "    'DELS diff',\n",
        "    'HQINS diff',\n",
        "    'HQDEL diff',\n",
        "    'PARENT']\n",
        "print('Total feature names: ',len(feature_names))\n",
        "for i in range(len(feature_names)):\n",
        "    print(i,feature_names[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p35ehKV3Kq0z"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self,filepath1,filepath2,verbose=True):\n",
        "        self.files = [filepath1,filepath2]\n",
        "        self.alignments=[]\n",
        "        self.labels=[]\n",
        "        self.verbose = verbose\n",
        "        self.max_lines = None\n",
        "        self.ties = 0\n",
        "        self.predictions = []\n",
        "        self.num_features = 1\n",
        "\n",
        "    def set_num_features(self,count):\n",
        "        self.num_features = count\n",
        "\n",
        "    def set_max_lines(self,lines):\n",
        "        '''Limit the dataset size to fit in RAM.'''\n",
        "        self.max_lines = lines\n",
        "        if self.verbose:\n",
        "            print('Maximum lines to load per file: %d'%lines)\n",
        "\n",
        "    def _count_lines_(self):\n",
        "        '''Show number of lines per input file.'''\n",
        "        count0 = 0\n",
        "        with gzip.open (self.files[0],'rt') as handle0:\n",
        "            for row in handle0:\n",
        "                count0 += 1\n",
        "        count1 = 0\n",
        "        with gzip.open(self.files[1],'rt') as handle1:\n",
        "            for row in handle1:\n",
        "                count1 += 1\n",
        "        minimum = min(count0,count1)\n",
        "        if self.verbose:\n",
        "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
        "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
        "        return minimum\n",
        "\n",
        "    def _load_line_(self,row):\n",
        "        '''Load data structure from one line of CSV file.'''\n",
        "        line = row.strip()\n",
        "        fields = line.split(',')\n",
        "        ints = [0] * self.num_features\n",
        "        # These fields come straight from the input file.\n",
        "        # These fields are grouped by which read they describe.\n",
        "        # P1 R1 = Parent 1, Read 1\n",
        "        ints[0] = int(fields[0]) # P1 R1 AS\n",
        "        ints[1] = int(fields[1]) # P1 R1 ED\n",
        "        P1R1_MAT = int(fields[2]) # P1 R1 MAT\n",
        "        ints[2] = int(fields[3]) # P1 R1 MM\n",
        "        ints[3] = int(fields[4]) # P1 R1 HQMM\n",
        "        ints[4] = int(fields[5]) # P1 R1 GO\n",
        "        ints[5] = int(fields[6]) # P1 R1 GE\n",
        "        ints[6] = int(fields[7]) # P1 R1 INS\n",
        "        ints[7] = int(fields[8]) # P1 R1 DELS\n",
        "        ints[8] = int(fields[9]) # P1 R1 HQINS\n",
        "        ints[9] = int(fields[10]) # P1 R1 HQDEL\n",
        "        #  = Parent 1, Read 2\n",
        "        ints[10] = int(fields[11]) # P1 R2 AS\n",
        "        ints[11] = int(fields[12]) # P1 R2 ED\n",
        "        P1R2_MAT = int(fields[13]) # P1 R2 MAT\n",
        "        ints[12] = int(fields[14]) # P1 R2 MM\n",
        "        ints[13] = int(fields[15]) # P1 R2 HQMM\n",
        "        ints[14] = int(fields[16]) # P1 R2 GO\n",
        "        ints[15] = int(fields[17]) # P1 R2 GE\n",
        "        ints[16] = int(fields[18]) # P1 R2 INS\n",
        "        ints[17] = int(fields[19]) # P1 R2 DELS\n",
        "        ints[18] = int(fields[20]) # P1 R2 HQINS\n",
        "        ints[19] = int(fields[21]) # P1 R2 HQDEL\n",
        "        # P2 R1 = Parent 2, Read 1\n",
        "        ints[20] = int(fields[22]) # P2 R1 AS\n",
        "        ints[21] = int(fields[23]) # P2 R1 ED\n",
        "        P2R1_MAT = int(fields[24]) # P2 R1 MAT\n",
        "        ints[22] = int(fields[25]) # P2 R1 MM\n",
        "        ints[23] = int(fields[26]) # P2 R1 HQMM\n",
        "        ints[24] = int(fields[27]) # P2 R1 GO\n",
        "        ints[25] = int(fields[28]) # P2 R1 GE\n",
        "        ints[26] = int(fields[29]) # P2 R1 INS\n",
        "        ints[27] = int(fields[30]) # P2 R1 DELS\n",
        "        ints[28] = int(fields[31]) # P2 R1 HQINS\n",
        "        ints[29] = int(fields[32]) # P2 R1 HQDEL\n",
        "        # P2 R2 = Parent 2, Read 2\n",
        "        ints[30] = int(fields[33]) # P2 R2 AS\n",
        "        ints[31] = int(fields[34]) # P2 R2 ED\n",
        "        P2R2_MAT = int(fields[35]) # P2 R2 MAT\n",
        "        ints[32] = int(fields[36]) # P2 R2 MM\n",
        "        ints[33] = int(fields[37]) # P2 R2 HQMM\n",
        "        ints[34] = int(fields[38]) # P2 R2 GO\n",
        "        ints[35] = int(fields[39]) # P2 R2 GE\n",
        "        ints[36] = int(fields[40]) # P2 R2 INS\n",
        "        ints[37] = int(fields[41]) # P2 R2 DELS\n",
        "        ints[38] = int(fields[42]) # P2 R2 HQINS\n",
        "        ints[39] = int(fields[43]) # P2 R2 HQDEL\n",
        "        # Fields that come in twos\n",
        "        R1_LEN = int(fields[44]) # R1 length (of read)\n",
        "        R2_LEN = int(fields[45]) # R2 length (of read)\n",
        "        P1_SPAN = int(fields[46]) # P1 span (of mapped read pair)\n",
        "        P2_SPAN = int(fields[47]) # P2 span (of mapped read pair)\n",
        "        # Read-wise differences\n",
        "        ints[40] = P2_SPAN-P1_SPAN # P2-P1 span diff\n",
        "        ints[41] = (ints[33]+ints[22])-(ints[11]+ints[0]) # AS diff\n",
        "        ints[42] = (ints[34]+ints[23])-(ints[12]+ints[1]) # ED diff\n",
        "        ints[43] = (P2R1_MAT+P2R2_MAT)-(P1R1_MAT+P1R2_MAT) # MAT diff\n",
        "        ints[44] = (ints[36]+ints[25])-(ints[14]+ints[3]) # MM diff\n",
        "        ints[45] = (ints[37]+ints[26])-(ints[15]+ints[4]) # HQMM diff\n",
        "        ints[46] = (ints[38]+ints[27])-(ints[16]+ints[5]) # GO diff\n",
        "        ints[47] = (ints[39]+ints[28])-(ints[17]+ints[6]) # GE diff\n",
        "        ints[48] = (ints[40]+ints[29])-(ints[18]+ints[7]) # INS diff\n",
        "        ints[49] = (ints[41]+ints[30])-(ints[19]+ints[8]) # DELS diff\n",
        "        ints[50] = (ints[42]+ints[31])-(ints[20]+ints[9]) # HQINS diff\n",
        "        ints[51] = (ints[43]+ints[32])-(ints[21]+ints[10]) # HQDEL diff\n",
        "        # The feature-extraction program populated a field\n",
        "        # to indicate which parent had higher alignment score.\n",
        "        # Values were 0=same, 1=parent1, 2=parent2.\n",
        "        # We change the values to -1=parent1, 0=unknown, +1=parent2\n",
        "        parent_choice = int(fields[48])\n",
        "        if parent_choice == 1:\n",
        "            ints[52] = -1  # not parent 2\n",
        "        elif parent_choice == 2:\n",
        "            ints[52] = 1  # is parent 2\n",
        "        elif parent_choice == 0:\n",
        "            ints[52] = 0\n",
        "        else:\n",
        "            raise Exception('Unrecognized parent choice:'+str(parent_choice))\n",
        "        # For fair comparison, force aligner to choose.\n",
        "        # We change 1 to 0, 2 to 1, and 0 to 1 or 2 randomly.\n",
        "        # TO DO: faster alternative to list.append() ???\n",
        "        parent_choice = int(fields[48])\n",
        "        if parent_choice == 1:\n",
        "            self.predictions.append(0)  # not parent 2\n",
        "        elif parent_choice == 2:\n",
        "            self.predictions.append(1)  # is parent 2\n",
        "        else: # parent_choice == 0:\n",
        "            self.ties += 1\n",
        "            guess = random.randint(0,1)\n",
        "            self.predictions.append(guess)\n",
        "        # The transcript that this read pair aligned to.\n",
        "        # This is for pipelines that only process reads that map\n",
        "        # to same transcript in both parents and (filter the others).\n",
        "        # Pipelines that assign reads to parent, regardless of which gene,\n",
        "        # should ignore this value. (It only reflects first parent map anyway.)\n",
        "        transcript_id = fields[49] # TO DO: where to put this?\n",
        "        self.alignments.append(ints)\n",
        "\n",
        "    def count_ties(self):\n",
        "        return self.ties\n",
        "\n",
        "    def load_full_train_set(self):\n",
        "        '''Load full train set (to be used for train and valiation).\n",
        "           Use set_max_lines() to leave some data for the test set.'''\n",
        "        minimum = 0\n",
        "        train_size = self.max_lines\n",
        "        if self.verbose:\n",
        "            print('Trying to load %d lines per file...'%train_size)\n",
        "        try:\n",
        "            handle0 = gzip.open(self.files[0],'rt')\n",
        "            handle1 = gzip.open(self.files[1],'rt')\n",
        "            # Associate label 0 with data from file 0. Same for 1.\n",
        "            for i in range(train_size):\n",
        "                row = next(handle0)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(0)\n",
        "                row = next(handle1)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(1)\n",
        "            handle0.close()\n",
        "            handle1.close()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print('Most likely, one file has too few reads.')\n",
        "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
        "\n",
        "    def show_examples(self,head=6):\n",
        "        head = min(head,len(self.alignments))\n",
        "        for i in range(head):\n",
        "            print('From '+self.files[self.labels[i]])\n",
        "            print('Score,Edit,MM,HQMM,GapOpen,GapExtend,INS,DELS,HQINS,HQDEL')\n",
        "            print(self.alignments[i][0:9])\n",
        "            print(self.alignments[i][10:19])\n",
        "            print(self.alignments[i][20:29])\n",
        "            print(self.alignments[i][30:39])\n",
        "            print('Parent choice:',self.alignments[i][52])\n",
        "\n",
        "    def get_X_y(self):\n",
        "        loaded = len(self.alignments)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        X_train = np.array(self.alignments[:divider])\n",
        "        y_train = np.array(self.labels[:divider])\n",
        "        X_valid = np.array(self.alignments[divider:])\n",
        "        y_valid = np.array(self.labels[divider:])\n",
        "        if self.verbose:\n",
        "            print('Full train set size = '+str(len(self.alignments)))\n",
        "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
        "        return X_train,y_train, X_valid,y_valid\n",
        "\n",
        "    def get_predictions(self):\n",
        "        loaded = len(self.predictions)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        y_train = self.predictions[:divider]\n",
        "        y_valid = self.predictions[divider:]\n",
        "        return y_train, y_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pcZVyvS_126",
        "outputId": "ce625043-5357-43a3-f8f3-0141e9f9a711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 14:04:34.258600\n",
            "Maximum lines to load per file: 1000000\n",
            "LOADING\n",
            "Trying to load 1000000 lines per file...\n",
            "Number of ties: 739279\n",
            "2023-11-16 14:05:29.468281\n"
          ]
        }
      ],
      "source": [
        "print(datetime.now())\n",
        "filepath0 = DATA_DIR+DATA_FILE_0\n",
        "filepath1 = DATA_DIR+DATA_FILE_1\n",
        "loader=DataLoader(filepath0,filepath1)\n",
        "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
        "loader.set_num_features(len(feature_names))\n",
        "print('LOADING')\n",
        "loader.load_full_train_set()\n",
        "print('Number of ties: %d' % loader.count_ties())\n",
        "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7uHn9Ib_129",
        "outputId": "96b6efd8-328e-4ed9-d898-49f14629a70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full train set size = 2000000\n",
            "Training/Validation partition: 1600000/400000\n",
            "X train shape: \n",
            "(1600000, 53)\n",
            "y train shape: \n",
            "(1600000,)\n"
          ]
        }
      ],
      "source": [
        "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
        "X_valid = None\n",
        "y_valid = None\n",
        "print('X train shape: ')\n",
        "print(np.shape(X_train))\n",
        "print('y train shape: ')\n",
        "print(np.shape(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDZ6siB_Kq04"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AwMbRjm0FLBF"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    rfc = RFC()\n",
        "    return rfc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VCzbj21FMpfU"
      },
      "outputs": [],
      "source": [
        "# docs: Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
        "def show_performance(y_valid, yhat_classes, yhat_pred):\n",
        "    accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
        "    precision = precision_score(y_valid, yhat_classes)*100.\n",
        "    recall = recall_score(y_valid, yhat_classes)*100.\n",
        "    sensitivity = recall_score(y_valid, yhat_classes, pos_label=1)*100.\n",
        "    specificity = recall_score(y_valid, yhat_classes, pos_label=0)*100.\n",
        "    f1 = f1_score(y_valid, yhat_classes)*100.\n",
        "    mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
        "    if yhat_pred is None:\n",
        "        # these stats are possible for probabilistic models only\n",
        "        auprc = 0.\n",
        "        auroc = 0.\n",
        "    else:\n",
        "        prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
        "        auprc = auc(prc_X,prc_Y)*100.\n",
        "        auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
        "    values,counts=np.unique(yhat_classes, return_counts=True)\n",
        "    print('Predictions: ', dict(zip(values, counts)))\n",
        "    print('Accuracy: %.2f%% F1: %.2f%% MCC: %.4f' % (accuracy,f1,mcc))\n",
        "    print('Precision: %.2f%% Recall: %.2f%% AUPRC: %.2f%%' % (precision,recall,auprc))\n",
        "    print('Sensitivity: %.2f%% Specificity: %.2f%% AUROC: %.2f%%' % (sensitivity,specificity,auroc))\n",
        "    return accuracy,precision,recall,f1,sensitivity,specificity,mcc,auprc,auroc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfgplJ3Ep8Vr"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HCWG_w9_13F",
        "outputId": "535027f5-97d5-4b27-8893-f84706b69d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 14:05:44.017222\n",
            "FOLD 1\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 555.6151416301727\n",
            "Distrib of scores: 0.5002179152031972 mean 0.40282478257096 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[144143  15624]\n",
            " [ 29025 131208]]\n",
            "Normalized matrix\n",
            " [[0.45044688 0.048825  ]\n",
            " [0.09070312 0.410025  ]]\n",
            "Predictions:  {0: 173168, 1: 146832}\n",
            "Accuracy: 86.05% F1: 85.46% MCC: 0.7235\n",
            "Precision: 89.36% Recall: 81.89% AUPRC: 94.09%\n",
            "Sensitivity: 81.89% Specificity: 90.22% AUROC: 93.93%\n",
            "2023-11-16 14:15:33.689287\n",
            "FOLD 2\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 583.5822606086731\n",
            "Distrib of scores: 0.4997961477350994 mean 0.40328723913979514 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[144297  15841]\n",
            " [ 28755 131107]]\n",
            "Normalized matrix\n",
            " [[0.45092813 0.04950313]\n",
            " [0.08985938 0.40970937]]\n",
            "Predictions:  {0: 173052, 1: 146948}\n",
            "Accuracy: 86.06% F1: 85.46% MCC: 0.7236\n",
            "Precision: 89.22% Recall: 82.01% AUPRC: 94.10%\n",
            "Sensitivity: 82.01% Specificity: 90.11% AUROC: 93.98%\n",
            "2023-11-16 14:25:50.161003\n",
            "FOLD 3\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 453.27588391304016\n",
            "Distrib of scores: 0.4992370329589854 mean 0.4032868417734375 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[144506  15617]\n",
            " [ 28850 131027]]\n",
            "Normalized matrix\n",
            " [[0.45158125 0.04880313]\n",
            " [0.09015625 0.40945938]]\n",
            "Predictions:  {0: 173356, 1: 146644}\n",
            "Accuracy: 86.10% F1: 85.49% MCC: 0.7245\n",
            "Precision: 89.35% Recall: 81.95% AUPRC: 94.18%\n",
            "Sensitivity: 81.95% Specificity: 90.25% AUROC: 94.05%\n",
            "2023-11-16 14:33:57.040098\n",
            "FOLD 4\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 637.8683848381042\n",
            "Distrib of scores: 0.5002908335072684 mean 0.4029340190758173 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[144548  15627]\n",
            " [ 28619 131206]]\n",
            "Normalized matrix\n",
            " [[0.4517125  0.04883437]\n",
            " [0.08943437 0.41001875]]\n",
            "Predictions:  {0: 173167, 1: 146833}\n",
            "Accuracy: 86.17% F1: 85.57% MCC: 0.7258\n",
            "Precision: 89.36% Recall: 82.09% AUPRC: 94.10%\n",
            "Sensitivity: 82.09% Specificity: 90.24% AUROC: 94.03%\n",
            "2023-11-16 14:45:06.583643\n",
            "FOLD 5\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 644.3343079090118\n",
            "Distrib of scores: 0.5005835331323597 mean 0.40337257983366465 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[143887  15910]\n",
            " [ 28877 131326]]\n",
            "Normalized matrix\n",
            " [[0.44964688 0.04971875]\n",
            " [0.09024063 0.41039375]]\n",
            "Predictions:  {0: 172764, 1: 147236}\n",
            "Accuracy: 86.00% F1: 85.43% MCC: 0.7225\n",
            "Precision: 89.19% Recall: 81.97% AUPRC: 94.07%\n",
            "Sensitivity: 81.97% Specificity: 90.04% AUROC: 93.95%\n",
            "2023-11-16 14:56:21.173835\n"
          ]
        }
      ],
      "source": [
        "print(datetime.now())\n",
        "FOLD = 0\n",
        "accuracyCV=list()\n",
        "precisionCV=list()\n",
        "recallCV=list()\n",
        "f1CV=list()\n",
        "sensitivityCV=list()\n",
        "specificityCV=list()\n",
        "mccCV=list()\n",
        "auprcCV=list()\n",
        "aurocCV=list()\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for train_index, test_index in kfold.split(X_train, y_train):\n",
        "    rfc_model=build_model()\n",
        "    FOLD += 1\n",
        "    print('FOLD', FOLD)\n",
        "    print('Training samples', len(train_index), 'Testing samples', len(test_index) )\n",
        "    print('Training...')\n",
        "    start_time = time.time()\n",
        "    rfc_model.fit(X_train[train_index], y_train[train_index])\n",
        "    end_time = time.time()\n",
        "    print('Elapsed seconds:', (end_time-start_time))\n",
        "\n",
        "    yhat_pairs=rfc_model.predict_proba(X_train[test_index])\n",
        "    yhat_pred=[pair[1] for pair in yhat_pairs]\n",
        "    yhat_classes=rfc_model.predict(X_train[test_index])\n",
        "\n",
        "    print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
        "    print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
        "    cm1 = confusion_matrix(y_train[test_index],yhat_classes)\n",
        "    print('Confusion matrix\\n',cm1)\n",
        "    cm2 = confusion_matrix(y_train[test_index],yhat_classes,normalize='all')\n",
        "    print('Normalized matrix\\n',cm2)\n",
        "\n",
        "    accuracy,precision,recall,f1,sensitivity,specificity,mcc,auprc,auroc=\\\n",
        "        show_performance(y_train[test_index], yhat_classes, yhat_pred)\n",
        "    accuracyCV.append   (accuracy)\n",
        "    precisionCV.append  (precision)\n",
        "    recallCV.append     (recall)\n",
        "    f1CV.append         (f1)\n",
        "    sensitivityCV.append(sensitivity)\n",
        "    specificityCV.append(specificity)\n",
        "    mccCV.append        (mcc)\n",
        "    auprcCV.append      (auprc)\n",
        "    aurocCV.append      (auroc)\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si8QbOpY_13G",
        "outputId": "b70a0c08-532c-4ea0-ab75-5adb2d555a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy    86.07843750000002 0.057177423975724485\n",
            "precision   89.2962358802902 0.0732873337099474\n",
            "recall      81.9843053824305 0.0684151342267327\n",
            "f1          85.48416630736499 0.04775945930510215\n",
            "sensitivity 81.9843053824305 0.0684151342267327\n",
            "specificity 90.17259046311247 0.08212936503414281\n",
            "mcc         0.7240004012577271 0.0011257551833727061\n",
            "auprc       94.10740676015534 0.0399507445399635\n",
            "auroc       93.98829128773268 0.04422801065799575\n"
          ]
        }
      ],
      "source": [
        "print('accuracy   ',   np.mean(accuracyCV),    np.std(accuracyCV))\n",
        "print('precision  ',  np.mean(precisionCV),   np.std(precisionCV))\n",
        "print('recall     ',     np.mean(recallCV),      np.std(recallCV))\n",
        "print('f1         ',         np.mean(f1CV),          np.std(f1CV))\n",
        "print('sensitivity',np.mean(sensitivityCV), np.std(sensitivityCV))\n",
        "print('specificity',np.mean(specificityCV), np.std(specificityCV))\n",
        "print('mcc        ',        np.mean(mccCV),         np.std(mccCV))\n",
        "print('auprc      ',      np.mean(auprcCV),       np.std(auprcCV))\n",
        "print('auroc      ',      np.mean(aurocCV),       np.std(aurocCV))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}