{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG-tGRnlFLA3"
      },
      "source": [
        "# Cross Validation\n",
        "Random Forest.\n",
        "\n",
        "Brassica, Bowtie, transcripts.\n",
        "\n",
        "Stats from Sorted.bam files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RmwUsVLFLA6",
        "outputId": "fade0a7e-14d5-4d9f-e169-557b768a8586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-19 23:53:25.252172\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlzN9OdsFWEU",
        "outputId": "996e5835-bd01-4d06-ba2f-81d2a23d5a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU device not found\n",
            "Running on CoLab\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    print('Running on CoLab')\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATA_DIR=PATH+'My Drive/data/IRP2/Brassica/Bowtie/'  # must end in \"/\"\n",
        "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print('Running on Mac')\n",
        "    DATA_DIR=\"Brassica/Bowtie/\"\n",
        "    MODEL_DIR=\"Models/\"\n",
        "SAVE_MODEL_FILENAME = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIAT2G5DYwvS",
        "outputId": "42fc84a6-1c07-4757-a29d-83f1fc747591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "sklearn 1.2.2\n"
          ]
        }
      ],
      "source": [
        "from platform import python_version\n",
        "print('Python',python_version())\n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42) # supposedly sets scikit-learn\n",
        "import pandas as pd  # for plotting\n",
        "import time # sleep function\n",
        "from os.path import isfile\n",
        "import gzip\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn   # pip install --upgrade scikit-learn\n",
        "print('sklearn',sklearn.__version__)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "\n",
        "EPOCHS=150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtqdpJOxFLBA"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnkpVKdMFLA-",
        "outputId": "a510f975-6247-451c-df17-bdc5f24c25c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory: /content/drive/My Drive/data/IRP2/Brassica/Bowtie/\n",
            "Data file 0 unfiltered.rapa_read_stats.csv.gz\n",
            "Data file 1 unfiltered.oleracea_read_stats.csv.gz\n",
            "Input lines for training: 1000000\n"
          ]
        }
      ],
      "source": [
        "MAX_LINES_TO_LOAD =    1000000 # training - 1M lines requires 2GB RAM\n",
        "#MAX_LINES_TO_LOAD =    10000 # use this for debugging\n",
        "\n",
        "VALID_PORTION = 0.20\n",
        "\n",
        "DATA_FILE_0 = 'unfiltered.rapa_read_stats.csv.gz'\n",
        "DATA_FILE_1 = 'unfiltered.oleracea_read_stats.csv.gz'\n",
        "\n",
        "print('Data directory: %s'%DATA_DIR)\n",
        "print('Data file 0 %s'%DATA_FILE_0)\n",
        "print('Data file 1 %s'%DATA_FILE_1)\n",
        "print('Input lines for training: %d'%MAX_LINES_TO_LOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUBLdpVEVQ3I",
        "outputId": "6313a850-797a-4bb7-9401-fc9eb6c772c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total feature names:  53\n",
            "0 P1 R1 AS\n",
            "1 P1 R1 ED\n",
            "2 P1 R1 MM\n",
            "3 P1 R1 HQMM\n",
            "4 P1 R1 GO\n",
            "5 P1 R1 GE\n",
            "6 P1 R1 INS\n",
            "7 P1 R1 DELS\n",
            "8 P1 R1 HQINS\n",
            "9 P1 R1 HQDEL\n",
            "10 P1 R2 AS\n",
            "11 P1 R2 ED\n",
            "12 P1 R2 MM\n",
            "13 P1 R2 HQMM\n",
            "14 P1 R2 GO\n",
            "15 P1 R2 GE\n",
            "16 P1 R2 INS\n",
            "17 P1 R2 DELS\n",
            "18 P1 R2 HQINS\n",
            "19 P1 R2 HQDEL\n",
            "20 P2 R1 AS\n",
            "21 P2 R1 ED\n",
            "22 P2 R1 MM\n",
            "23 P2 R1 HQMM\n",
            "24 P2 R1 GO\n",
            "25 P2 R1 GE\n",
            "26 P2 R1 INS\n",
            "27 P2 R1 DELS\n",
            "28 P2 R1 HQINS\n",
            "29 P2 R1 HQDEL\n",
            "30 P2 R2 AS\n",
            "31 P2 R2 ED\n",
            "32 P2 R2 MM\n",
            "33 P2 R2 HQMM\n",
            "34 P2 R2 GO\n",
            "35 P2 R2 GE\n",
            "36 P2 R2 INS\n",
            "37 P2 R2 DELS\n",
            "38 P2 R2 HQINS\n",
            "39 P2 R2 HQDEL\n",
            "40 Span diff\n",
            "41 AS diff\n",
            "42 ED diff\n",
            "43 MAT diff\n",
            "44 MM diff\n",
            "45 HQMM diff\n",
            "46 GO diff\n",
            "47 GE diff\n",
            "48 INS diff\n",
            "49 DELS diff\n",
            "50 HQINS diff\n",
            "51 HQDEL diff\n",
            "52 PARENT\n"
          ]
        }
      ],
      "source": [
        "# P1 parent 1\n",
        "# R1 read 1\n",
        "# PS primary or secondary\n",
        "# AS bowtie alignment score (0 is best)\n",
        "# ED edit distance\n",
        "# MM mismatch count\n",
        "# GO gap open count\n",
        "# GE gap extend count\n",
        "feature_names = [\n",
        "    'P1 R1 AS',\n",
        "    'P1 R1 ED',\n",
        "    ##'P1 R1 MAT',\n",
        "    'P1 R1 MM',\n",
        "    'P1 R1 HQMM',\n",
        "    'P1 R1 GO',\n",
        "    'P1 R1 GE',\n",
        "    'P1 R1 INS',\n",
        "    'P1 R1 DELS',\n",
        "    'P1 R1 HQINS',\n",
        "    'P1 R1 HQDEL',\n",
        "    'P1 R2 AS',\n",
        "    'P1 R2 ED',\n",
        "    ##'P1 R2 MAT',\n",
        "    'P1 R2 MM',\n",
        "    'P1 R2 HQMM',\n",
        "    'P1 R2 GO',\n",
        "    'P1 R2 GE',\n",
        "    'P1 R2 INS',\n",
        "    'P1 R2 DELS',\n",
        "    'P1 R2 HQINS',\n",
        "    'P1 R2 HQDEL',\n",
        "    'P2 R1 AS',\n",
        "    'P2 R1 ED',\n",
        "    ##'P2 R1 MAT',\n",
        "    'P2 R1 MM',\n",
        "    'P2 R1 HQMM',\n",
        "    'P2 R1 GO',\n",
        "    'P2 R1 GE',\n",
        "    'P2 R1 INS',\n",
        "    'P2 R1 DELS',\n",
        "    'P2 R1 HQINS',\n",
        "    'P2 R1 HQDEL',\n",
        "    'P2 R2 AS',\n",
        "    'P2 R2 ED',\n",
        "    ##'P2 R2 MAT',\n",
        "    'P2 R2 MM',\n",
        "    'P2 R2 HQMM',\n",
        "    'P2 R2 GO',\n",
        "    'P2 R2 GE',\n",
        "    'P2 R2 INS',\n",
        "    'P2 R2 DELS',\n",
        "    'P2 R2 HQINS',\n",
        "    'P2 R2 HQDEL',\n",
        "    ##'R1 length',\n",
        "    ##'R2 length',\n",
        "    ##'P1 span',\n",
        "    ##'P2 span',\n",
        "    'Span diff',\n",
        "    'AS diff',\n",
        "    'ED diff',\n",
        "    'MAT diff',\n",
        "    'MM diff',\n",
        "    'HQMM diff',\n",
        "    'GO diff',\n",
        "    'GE diff',\n",
        "    'INS diff',\n",
        "    'DELS diff',\n",
        "    'HQINS diff',\n",
        "    'HQDEL diff',\n",
        "    'PARENT']\n",
        "print('Total feature names: ',len(feature_names))\n",
        "for i in range(len(feature_names)):\n",
        "    print(i,feature_names[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p35ehKV3Kq0z"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self,filepath1,filepath2,verbose=True):\n",
        "        self.files = [filepath1,filepath2]\n",
        "        self.alignments=[]\n",
        "        self.labels=[]\n",
        "        self.verbose = verbose\n",
        "        self.max_lines = None\n",
        "        self.ties = 0\n",
        "        self.predictions = []\n",
        "        self.num_features = 1\n",
        "\n",
        "    def set_num_features(self,count):\n",
        "        self.num_features = count\n",
        "\n",
        "    def set_max_lines(self,lines):\n",
        "        '''Limit the dataset size to fit in RAM.'''\n",
        "        self.max_lines = lines\n",
        "        if self.verbose:\n",
        "            print('Maximum lines to load per file: %d'%lines)\n",
        "\n",
        "    def _count_lines_(self):\n",
        "        '''Show number of lines per input file.'''\n",
        "        count0 = 0\n",
        "        with gzip.open (self.files[0],'rt') as handle0:\n",
        "            for row in handle0:\n",
        "                count0 += 1\n",
        "        count1 = 0\n",
        "        with gzip.open(self.files[1],'rt') as handle1:\n",
        "            for row in handle1:\n",
        "                count1 += 1\n",
        "        minimum = min(count0,count1)\n",
        "        if self.verbose:\n",
        "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
        "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
        "        return minimum\n",
        "\n",
        "    def _load_line_(self,row):\n",
        "        '''Load data structure from one line of CSV file.'''\n",
        "        line = row.strip()\n",
        "        fields = line.split(',')\n",
        "        ints = [0] * self.num_features\n",
        "        # These fields come straight from the input file.\n",
        "        # These fields are grouped by which read they describe.\n",
        "        # P1 R1 = Parent 1, Read 1\n",
        "        ints[0] = int(fields[0]) # P1 R1 AS\n",
        "        ints[1] = int(fields[1]) # P1 R1 ED\n",
        "        P1R1_MAT = int(fields[2]) # P1 R1 MAT\n",
        "        ints[2] = int(fields[3]) # P1 R1 MM\n",
        "        ints[3] = int(fields[4]) # P1 R1 HQMM\n",
        "        ints[4] = int(fields[5]) # P1 R1 GO\n",
        "        ints[5] = int(fields[6]) # P1 R1 GE\n",
        "        ints[6] = int(fields[7]) # P1 R1 INS\n",
        "        ints[7] = int(fields[8]) # P1 R1 DELS\n",
        "        ints[8] = int(fields[9]) # P1 R1 HQINS\n",
        "        ints[9] = int(fields[10]) # P1 R1 HQDEL\n",
        "        #  = Parent 1, Read 2\n",
        "        ints[10] = int(fields[11]) # P1 R2 AS\n",
        "        ints[11] = int(fields[12]) # P1 R2 ED\n",
        "        P1R2_MAT = int(fields[13]) # P1 R2 MAT\n",
        "        ints[12] = int(fields[14]) # P1 R2 MM\n",
        "        ints[13] = int(fields[15]) # P1 R2 HQMM\n",
        "        ints[14] = int(fields[16]) # P1 R2 GO\n",
        "        ints[15] = int(fields[17]) # P1 R2 GE\n",
        "        ints[16] = int(fields[18]) # P1 R2 INS\n",
        "        ints[17] = int(fields[19]) # P1 R2 DELS\n",
        "        ints[18] = int(fields[20]) # P1 R2 HQINS\n",
        "        ints[19] = int(fields[21]) # P1 R2 HQDEL\n",
        "        # P2 R1 = Parent 2, Read 1\n",
        "        ints[20] = int(fields[22]) # P2 R1 AS\n",
        "        ints[21] = int(fields[23]) # P2 R1 ED\n",
        "        P2R1_MAT = int(fields[24]) # P2 R1 MAT\n",
        "        ints[22] = int(fields[25]) # P2 R1 MM\n",
        "        ints[23] = int(fields[26]) # P2 R1 HQMM\n",
        "        ints[24] = int(fields[27]) # P2 R1 GO\n",
        "        ints[25] = int(fields[28]) # P2 R1 GE\n",
        "        ints[26] = int(fields[29]) # P2 R1 INS\n",
        "        ints[27] = int(fields[30]) # P2 R1 DELS\n",
        "        ints[28] = int(fields[31]) # P2 R1 HQINS\n",
        "        ints[29] = int(fields[32]) # P2 R1 HQDEL\n",
        "        # P2 R2 = Parent 2, Read 2\n",
        "        ints[30] = int(fields[33]) # P2 R2 AS\n",
        "        ints[31] = int(fields[34]) # P2 R2 ED\n",
        "        P2R2_MAT = int(fields[35]) # P2 R2 MAT\n",
        "        ints[32] = int(fields[36]) # P2 R2 MM\n",
        "        ints[33] = int(fields[37]) # P2 R2 HQMM\n",
        "        ints[34] = int(fields[38]) # P2 R2 GO\n",
        "        ints[35] = int(fields[39]) # P2 R2 GE\n",
        "        ints[36] = int(fields[40]) # P2 R2 INS\n",
        "        ints[37] = int(fields[41]) # P2 R2 DELS\n",
        "        ints[38] = int(fields[42]) # P2 R2 HQINS\n",
        "        ints[39] = int(fields[43]) # P2 R2 HQDEL\n",
        "        # Fields that come in twos\n",
        "        R1_LEN = int(fields[44]) # R1 length (of read)\n",
        "        R2_LEN = int(fields[45]) # R2 length (of read)\n",
        "        P1_SPAN = int(fields[46]) # P1 span (of mapped read pair)\n",
        "        P2_SPAN = int(fields[47]) # P2 span (of mapped read pair)\n",
        "        # Read-wise differences\n",
        "        ints[40] = P2_SPAN-P1_SPAN # P2-P1 span diff\n",
        "        ints[41] = (ints[33]+ints[22])-(ints[11]+ints[0]) # AS diff\n",
        "        ints[42] = (ints[34]+ints[23])-(ints[12]+ints[1]) # ED diff\n",
        "        ints[43] = (P2R1_MAT+P2R2_MAT)-(P1R1_MAT+P1R2_MAT) # MAT diff\n",
        "        ints[44] = (ints[36]+ints[25])-(ints[14]+ints[3]) # MM diff\n",
        "        ints[45] = (ints[37]+ints[26])-(ints[15]+ints[4]) # HQMM diff\n",
        "        ints[46] = (ints[38]+ints[27])-(ints[16]+ints[5]) # GO diff\n",
        "        ints[47] = (ints[39]+ints[28])-(ints[17]+ints[6]) # GE diff\n",
        "        ints[48] = (ints[40]+ints[29])-(ints[18]+ints[7]) # INS diff\n",
        "        ints[49] = (ints[41]+ints[30])-(ints[19]+ints[8]) # DELS diff\n",
        "        ints[50] = (ints[42]+ints[31])-(ints[20]+ints[9]) # HQINS diff\n",
        "        ints[51] = (ints[43]+ints[32])-(ints[21]+ints[10]) # HQDEL diff\n",
        "        # The feature-extraction program populated a field\n",
        "        # to indicate which parent had higher alignment score.\n",
        "        # Values were 0=same, 1=parent1, 2=parent2.\n",
        "        # We change the values to -1=parent1, 0=unknown, +1=parent2\n",
        "        parent_choice = int(fields[48])\n",
        "        if parent_choice == 1:\n",
        "            ints[52] = -1  # not parent 2\n",
        "        elif parent_choice == 2:\n",
        "            ints[52] = 1  # is parent 2\n",
        "        elif parent_choice == 0:\n",
        "            ints[52] = 0\n",
        "        else:\n",
        "            raise Exception('Unrecognized parent choice:'+str(parent_choice))\n",
        "        # For fair comparison, force aligner to choose.\n",
        "        # We change 1 to 0, 2 to 1, and 0 to 1 or 2 randomly.\n",
        "        # TO DO: faster alternative to list.append() ???\n",
        "        parent_choice = int(fields[48])\n",
        "        if parent_choice == 1:\n",
        "            self.predictions.append(0)  # not parent 2\n",
        "        elif parent_choice == 2:\n",
        "            self.predictions.append(1)  # is parent 2\n",
        "        else: # parent_choice == 0:\n",
        "            self.ties += 1\n",
        "            guess = random.randint(0,1)\n",
        "            self.predictions.append(guess)\n",
        "        # The transcript that this read pair aligned to.\n",
        "        # This is for pipelines that only process reads that map\n",
        "        # to same transcript in both parents and (filter the others).\n",
        "        # Pipelines that assign reads to parent, regardless of which gene,\n",
        "        # should ignore this value. (It only reflects first parent map anyway.)\n",
        "        transcript_id = fields[49] # TO DO: where to put this?\n",
        "        self.alignments.append(ints)\n",
        "\n",
        "    def count_ties(self):\n",
        "        return self.ties\n",
        "\n",
        "    def load_full_train_set(self):\n",
        "        '''Load full train set (to be used for train and valiation).\n",
        "           Use set_max_lines() to leave some data for the test set.'''\n",
        "        minimum = 0\n",
        "        train_size = self.max_lines\n",
        "        if self.verbose:\n",
        "            print('Trying to load %d lines per file...'%train_size)\n",
        "        try:\n",
        "            handle0 = gzip.open(self.files[0],'rt')\n",
        "            handle1 = gzip.open(self.files[1],'rt')\n",
        "            # Associate label 0 with data from file 0. Same for 1.\n",
        "            for i in range(train_size):\n",
        "                row = next(handle0)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(0)\n",
        "                row = next(handle1)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(1)\n",
        "            handle0.close()\n",
        "            handle1.close()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print('Most likely, one file has too few reads.')\n",
        "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
        "\n",
        "    def show_examples(self,head=6):\n",
        "        head = min(head,len(self.alignments))\n",
        "        for i in range(head):\n",
        "            print('From '+self.files[self.labels[i]])\n",
        "            print('Score,Edit,MM,HQMM,GapOpen,GapExtend,INS,DELS,HQINS,HQDEL')\n",
        "            print(self.alignments[i][0:9])\n",
        "            print(self.alignments[i][10:19])\n",
        "            print(self.alignments[i][20:29])\n",
        "            print(self.alignments[i][30:39])\n",
        "            print('Parent choice:',self.alignments[i][52])\n",
        "\n",
        "    def get_X_y(self):\n",
        "        loaded = len(self.alignments)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        X_train = np.array(self.alignments[:divider])\n",
        "        y_train = np.array(self.labels[:divider])\n",
        "        X_valid = np.array(self.alignments[divider:])\n",
        "        y_valid = np.array(self.labels[divider:])\n",
        "        if self.verbose:\n",
        "            print('Full train set size = '+str(len(self.alignments)))\n",
        "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
        "        return X_train,y_train, X_valid,y_valid\n",
        "\n",
        "    def get_predictions(self):\n",
        "        loaded = len(self.predictions)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        y_train = self.predictions[:divider]\n",
        "        y_valid = self.predictions[divider:]\n",
        "        return y_train, y_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pcZVyvS_126",
        "outputId": "57d2a507-bd12-4147-9634-37ba30a90e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-19 23:53:48.798722\n",
            "Maximum lines to load per file: 1000000\n",
            "LOADING\n",
            "Trying to load 1000000 lines per file...\n",
            "Number of ties: 63323\n",
            "2023-11-19 23:54:32.577979\n"
          ]
        }
      ],
      "source": [
        "print(datetime.now())\n",
        "filepath0 = DATA_DIR+DATA_FILE_0\n",
        "filepath1 = DATA_DIR+DATA_FILE_1\n",
        "loader=DataLoader(filepath0,filepath1)\n",
        "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
        "loader.set_num_features(len(feature_names))\n",
        "print('LOADING')\n",
        "loader.load_full_train_set()\n",
        "print('Number of ties: %d' % loader.count_ties())\n",
        "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7uHn9Ib_129",
        "outputId": "4d2ce790-4a82-4ab0-f7d7-4abc7f5ebce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full train set size = 2000000\n",
            "Training/Validation partition: 1600000/400000\n",
            "X train shape: \n",
            "(1600000, 53)\n",
            "y train shape: \n",
            "(1600000,)\n"
          ]
        }
      ],
      "source": [
        "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
        "X_valid = None\n",
        "y_valid = None\n",
        "print('X train shape: ')\n",
        "print(np.shape(X_train))\n",
        "print('y train shape: ')\n",
        "print(np.shape(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDZ6siB_Kq04"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AwMbRjm0FLBF"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    rfc = RFC()\n",
        "    return rfc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VCzbj21FMpfU"
      },
      "outputs": [],
      "source": [
        "# docs: Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
        "def show_performance(y_valid, yhat_classes, yhat_pred):\n",
        "    accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
        "    precision = precision_score(y_valid, yhat_classes)*100.\n",
        "    recall = recall_score(y_valid, yhat_classes)*100.\n",
        "    sensitivity = recall_score(y_valid, yhat_classes, pos_label=1)*100.\n",
        "    specificity = recall_score(y_valid, yhat_classes, pos_label=0)*100.\n",
        "    f1 = f1_score(y_valid, yhat_classes)*100.\n",
        "    mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
        "    if yhat_pred is None:\n",
        "        # these stats are possible for probabilistic models only\n",
        "        auprc = 0.\n",
        "        auroc = 0.\n",
        "    else:\n",
        "        prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
        "        auprc = auc(prc_X,prc_Y)*100.\n",
        "        auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
        "    values,counts=np.unique(yhat_classes, return_counts=True)\n",
        "    print('Predictions: ', dict(zip(values, counts)))\n",
        "    print('Accuracy: %.2f%% F1: %.2f%% MCC: %.4f' % (accuracy,f1,mcc))\n",
        "    print('Precision: %.2f%% Recall: %.2f%% AUPRC: %.2f%%' % (precision,recall,auprc))\n",
        "    print('Sensitivity: %.2f%% Specificity: %.2f%% AUROC: %.2f%%' % (sensitivity,specificity,auroc))\n",
        "    return accuracy,precision,recall,f1,sensitivity,specificity,mcc,auprc,auroc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfgplJ3Ep8Vr"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HCWG_w9_13F",
        "outputId": "6b906422-563b-41f9-edab-21bf34855565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-19 23:54:46.378664\n",
            "FOLD 1\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 365.4355089664459\n",
            "Distrib of scores: 0.5006668482147052 mean 0.45575769021191603 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[151231   8536]\n",
            " [ 11676 148557]]\n",
            "Normalized matrix\n",
            " [[0.47259688 0.026675  ]\n",
            " [0.0364875  0.46424063]]\n",
            "Predictions:  {0: 162907, 1: 157093}\n",
            "Accuracy: 93.68% F1: 93.63% MCC: 0.8738\n",
            "Precision: 94.57% Recall: 92.71% AUPRC: 98.31%\n",
            "Sensitivity: 92.71% Specificity: 94.66% AUROC: 98.28%\n",
            "2023-11-20 00:01:15.896850\n",
            "FOLD 2\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 476.1456160545349\n",
            "Distrib of scores: 0.49955863456198624 mean 0.4563639304697179 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[151722   8416]\n",
            " [ 11497 148365]]\n",
            "Normalized matrix\n",
            " [[0.47413125 0.0263    ]\n",
            " [0.03592812 0.46364062]]\n",
            "Predictions:  {0: 163219, 1: 156781}\n",
            "Accuracy: 93.78% F1: 93.71% MCC: 0.8757\n",
            "Precision: 94.63% Recall: 92.81% AUPRC: 98.32%\n",
            "Sensitivity: 92.81% Specificity: 94.74% AUROC: 98.30%\n",
            "2023-11-20 00:09:37.016480\n",
            "FOLD 3\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 342.67890977859497\n",
            "Distrib of scores: 0.49920723726013627 mean 0.45612823143216397 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[151569   8554]\n",
            " [ 11779 148098]]\n",
            "Normalized matrix\n",
            " [[0.47365313 0.02673125]\n",
            " [0.03680937 0.46280625]]\n",
            "Predictions:  {0: 163348, 1: 156652}\n",
            "Accuracy: 93.65% F1: 93.58% MCC: 0.8731\n",
            "Precision: 94.54% Recall: 92.63% AUPRC: 98.28%\n",
            "Sensitivity: 92.63% Specificity: 94.66% AUROC: 98.27%\n",
            "2023-11-20 00:15:43.502683\n",
            "FOLD 4\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 584.6612212657928\n",
            "Distrib of scores: 0.49914675675084225 mean 0.45600899245944293 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[151794   8381]\n",
            " [ 11611 148214]]\n",
            "Normalized matrix\n",
            " [[0.47435625 0.02619062]\n",
            " [0.03628438 0.46316875]]\n",
            "Predictions:  {0: 163405, 1: 156595}\n",
            "Accuracy: 93.75% F1: 93.68% MCC: 0.8752\n",
            "Precision: 94.65% Recall: 92.74% AUPRC: 98.33%\n",
            "Sensitivity: 92.74% Specificity: 94.77% AUROC: 98.32%\n",
            "2023-11-20 00:25:52.793724\n",
            "FOLD 5\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 407.16016817092896\n",
            "Distrib of scores: 0.5000164283674028 mean 0.45619993164784706 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[151411   8386]\n",
            " [ 11786 148417]]\n",
            "Normalized matrix\n",
            " [[0.47315937 0.02620625]\n",
            " [0.03683125 0.46380312]]\n",
            "Predictions:  {0: 163197, 1: 156803}\n",
            "Accuracy: 93.70% F1: 93.64% MCC: 0.8741\n",
            "Precision: 94.65% Recall: 92.64% AUPRC: 98.33%\n",
            "Sensitivity: 92.64% Specificity: 94.75% AUROC: 98.30%\n",
            "2023-11-20 00:33:04.878325\n"
          ]
        }
      ],
      "source": [
        "print(datetime.now())\n",
        "FOLD = 0\n",
        "accuracyCV=list()\n",
        "precisionCV=list()\n",
        "recallCV=list()\n",
        "f1CV=list()\n",
        "sensitivityCV=list()\n",
        "specificityCV=list()\n",
        "mccCV=list()\n",
        "auprcCV=list()\n",
        "aurocCV=list()\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for train_index, test_index in kfold.split(X_train, y_train):\n",
        "    rfc_model=build_model()\n",
        "    FOLD += 1\n",
        "    print('FOLD', FOLD)\n",
        "    print('Training samples', len(train_index), 'Testing samples', len(test_index) )\n",
        "    print('Training...')\n",
        "    start_time = time.time()\n",
        "    rfc_model.fit(X_train[train_index], y_train[train_index])\n",
        "    end_time = time.time()\n",
        "    print('Elapsed seconds:', (end_time-start_time))\n",
        "\n",
        "    yhat_pairs=rfc_model.predict_proba(X_train[test_index])\n",
        "    yhat_pred=[pair[1] for pair in yhat_pairs]\n",
        "    yhat_classes=rfc_model.predict(X_train[test_index])\n",
        "\n",
        "    print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
        "    print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
        "    cm1 = confusion_matrix(y_train[test_index],yhat_classes)\n",
        "    print('Confusion matrix\\n',cm1)\n",
        "    cm2 = confusion_matrix(y_train[test_index],yhat_classes,normalize='all')\n",
        "    print('Normalized matrix\\n',cm2)\n",
        "\n",
        "    accuracy,precision,recall,f1,sensitivity,specificity,mcc,auprc,auroc=\\\n",
        "        show_performance(y_train[test_index], yhat_classes, yhat_pred)\n",
        "    accuracyCV.append   (accuracy)\n",
        "    precisionCV.append  (precision)\n",
        "    recallCV.append     (recall)\n",
        "    f1CV.append         (f1)\n",
        "    sensitivityCV.append(sensitivity)\n",
        "    specificityCV.append(specificity)\n",
        "    mccCV.append        (mcc)\n",
        "    auprcCV.append      (auprc)\n",
        "    aurocCV.append      (auroc)\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si8QbOpY_13G",
        "outputId": "c0b1c83f-3f04-46c3-f584-17c3d5dfb174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy    93.71112500000001 0.04753148627488913\n",
            "precision   94.60752654743838 0.04589993414694339\n",
            "recall      92.70640159469654 0.06435321514488546\n",
            "f1          93.64730561725743 0.04629575907326742\n",
            "sensitivity 92.70640159469654 0.06435321514488546\n",
            "specificity 94.71585976903147 0.04819700648918557\n",
            "mcc         0.8743991423723555 0.0009453735899629434\n",
            "auprc       98.314403304101 0.018389323408974104\n",
            "auroc       98.29079512865106 0.018598176616014716\n"
          ]
        }
      ],
      "source": [
        "print('accuracy   ',   np.mean(accuracyCV),    np.std(accuracyCV))\n",
        "print('precision  ',  np.mean(precisionCV),   np.std(precisionCV))\n",
        "print('recall     ',     np.mean(recallCV),      np.std(recallCV))\n",
        "print('f1         ',         np.mean(f1CV),          np.std(f1CV))\n",
        "print('sensitivity',np.mean(sensitivityCV), np.std(sensitivityCV))\n",
        "print('specificity',np.mean(specificityCV), np.std(specificityCV))\n",
        "print('mcc        ',        np.mean(mccCV),         np.std(mccCV))\n",
        "print('auprc      ',      np.mean(auprcCV),       np.std(auprcCV))\n",
        "print('auroc      ',      np.mean(aurocCV),       np.std(aurocCV))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}