{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG-tGRnlFLA3"
      },
      "source": [
        "# Cross Validation\n",
        "Random Forest.\n",
        "\n",
        "Equus, hisat, genomes.\n",
        "\n",
        "Stats from Sorted.bam files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RmwUsVLFLA6",
        "outputId": "0bf90a1f-c024-4bfc-9dd0-c5e17b8fbb6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 15:47:35.104968\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlzN9OdsFWEU",
        "outputId": "117af29f-9a3d-4b92-ccf1-3d049d098c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU device not found\n",
            "Running on CoLab\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "dt='float32'\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "tf.random.set_seed(42) # supposedly leads to reproducible results\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    print('Running on CoLab')\n",
        "    PATH='/content/drive/'\n",
        "    drive.mount(PATH)\n",
        "    DATA_DIR=PATH+'My Drive/data/IRP2/Equus/HiSat/'  # must end in \"/\"\n",
        "    MODEL_DIR=PATH+'My Drive/data/IRP2/Models/'  # must end in \"/\"\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print('Running on Mac')\n",
        "    DATA_DIR=\"Equus/HiSat/\"\n",
        "    MODEL_DIR=\"Models/\"\n",
        "SAVE_MODEL_FILENAME = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIAT2G5DYwvS",
        "outputId": "da8e6d69-4655-4d2c-e0fe-5f53b6ce8588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "sklearn 1.2.2\n"
          ]
        }
      ],
      "source": [
        "from platform import python_version\n",
        "print('Python',python_version())\n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42) # supposedly sets scikit-learn\n",
        "import pandas as pd  # for plotting\n",
        "import time # sleep function\n",
        "from os.path import isfile\n",
        "import gzip\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn   # pip install --upgrade scikit-learn\n",
        "print('sklearn',sklearn.__version__)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "\n",
        "EPOCHS=150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtqdpJOxFLBA"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnkpVKdMFLA-",
        "outputId": "82f0e4f5-9091-4c08-e9ed-c510384454d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory: /content/drive/My Drive/data/IRP2/Equus/HiSat/\n",
            "Data file 0 unfiltered.asinus_read_stats.csv.gz\n",
            "Data file 1 unfiltered.caballus_read_stats.csv.gz\n",
            "Input lines for training: 1000000\n"
          ]
        }
      ],
      "source": [
        "MAX_LINES_TO_LOAD =    1000000 # training - 1M lines requires 2GB RAM\n",
        "#MAX_LINES_TO_LOAD =    10000 # use this for debugging\n",
        "\n",
        "VALID_PORTION = 0.20\n",
        "\n",
        "DATA_FILE_0 = 'unfiltered.asinus_read_stats.csv.gz'\n",
        "DATA_FILE_1 = 'unfiltered.caballus_read_stats.csv.gz'\n",
        "\n",
        "print('Data directory: %s'%DATA_DIR)\n",
        "print('Data file 0 %s'%DATA_FILE_0)\n",
        "print('Data file 1 %s'%DATA_FILE_1)\n",
        "print('Input lines for training: %d'%MAX_LINES_TO_LOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUBLdpVEVQ3I",
        "outputId": "f171d346-fcff-4fc6-ca0b-c5eab552ceb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total feature names:  53\n",
            "0 P1 R1 AS\n",
            "1 P1 R1 ED\n",
            "2 P1 R1 MM\n",
            "3 P1 R1 HQMM\n",
            "4 P1 R1 GO\n",
            "5 P1 R1 GE\n",
            "6 P1 R1 INS\n",
            "7 P1 R1 DELS\n",
            "8 P1 R1 HQINS\n",
            "9 P1 R1 HQDEL\n",
            "10 P1 R2 AS\n",
            "11 P1 R2 ED\n",
            "12 P1 R2 MM\n",
            "13 P1 R2 HQMM\n",
            "14 P1 R2 GO\n",
            "15 P1 R2 GE\n",
            "16 P1 R2 INS\n",
            "17 P1 R2 DELS\n",
            "18 P1 R2 HQINS\n",
            "19 P1 R2 HQDEL\n",
            "20 P2 R1 AS\n",
            "21 P2 R1 ED\n",
            "22 P2 R1 MM\n",
            "23 P2 R1 HQMM\n",
            "24 P2 R1 GO\n",
            "25 P2 R1 GE\n",
            "26 P2 R1 INS\n",
            "27 P2 R1 DELS\n",
            "28 P2 R1 HQINS\n",
            "29 P2 R1 HQDEL\n",
            "30 P2 R2 AS\n",
            "31 P2 R2 ED\n",
            "32 P2 R2 MM\n",
            "33 P2 R2 HQMM\n",
            "34 P2 R2 GO\n",
            "35 P2 R2 GE\n",
            "36 P2 R2 INS\n",
            "37 P2 R2 DELS\n",
            "38 P2 R2 HQINS\n",
            "39 P2 R2 HQDEL\n",
            "40 Span diff\n",
            "41 AS diff\n",
            "42 ED diff\n",
            "43 MAT diff\n",
            "44 MM diff\n",
            "45 HQMM diff\n",
            "46 GO diff\n",
            "47 GE diff\n",
            "48 INS diff\n",
            "49 DELS diff\n",
            "50 HQINS diff\n",
            "51 HQDEL diff\n",
            "52 PARENT\n"
          ]
        }
      ],
      "source": [
        "# P1 parent 1\n",
        "# R1 read 1\n",
        "# PS primary or secondary\n",
        "# AS bowtie alignment score (0 is best)\n",
        "# ED edit distance\n",
        "# MM mismatch count\n",
        "# GO gap open count\n",
        "# GE gap extend count\n",
        "feature_names = [\n",
        "    'P1 R1 AS',\n",
        "    'P1 R1 ED',\n",
        "    ##'P1 R1 MAT',\n",
        "    'P1 R1 MM',\n",
        "    'P1 R1 HQMM',\n",
        "    'P1 R1 GO',\n",
        "    'P1 R1 GE',\n",
        "    'P1 R1 INS',\n",
        "    'P1 R1 DELS',\n",
        "    'P1 R1 HQINS',\n",
        "    'P1 R1 HQDEL',\n",
        "    'P1 R2 AS',\n",
        "    'P1 R2 ED',\n",
        "    ##'P1 R2 MAT',\n",
        "    'P1 R2 MM',\n",
        "    'P1 R2 HQMM',\n",
        "    'P1 R2 GO',\n",
        "    'P1 R2 GE',\n",
        "    'P1 R2 INS',\n",
        "    'P1 R2 DELS',\n",
        "    'P1 R2 HQINS',\n",
        "    'P1 R2 HQDEL',\n",
        "    'P2 R1 AS',\n",
        "    'P2 R1 ED',\n",
        "    ##'P2 R1 MAT',\n",
        "    'P2 R1 MM',\n",
        "    'P2 R1 HQMM',\n",
        "    'P2 R1 GO',\n",
        "    'P2 R1 GE',\n",
        "    'P2 R1 INS',\n",
        "    'P2 R1 DELS',\n",
        "    'P2 R1 HQINS',\n",
        "    'P2 R1 HQDEL',\n",
        "    'P2 R2 AS',\n",
        "    'P2 R2 ED',\n",
        "    ##'P2 R2 MAT',\n",
        "    'P2 R2 MM',\n",
        "    'P2 R2 HQMM',\n",
        "    'P2 R2 GO',\n",
        "    'P2 R2 GE',\n",
        "    'P2 R2 INS',\n",
        "    'P2 R2 DELS',\n",
        "    'P2 R2 HQINS',\n",
        "    'P2 R2 HQDEL',\n",
        "    ##'R1 length',\n",
        "    ##'R2 length',\n",
        "    ##'P1 span',\n",
        "    ##'P2 span',\n",
        "    'Span diff',\n",
        "    'AS diff',\n",
        "    'ED diff',\n",
        "    'MAT diff',\n",
        "    'MM diff',\n",
        "    'HQMM diff',\n",
        "    'GO diff',\n",
        "    'GE diff',\n",
        "    'INS diff',\n",
        "    'DELS diff',\n",
        "    'HQINS diff',\n",
        "    'HQDEL diff',\n",
        "    'PARENT']\n",
        "print('Total feature names: ',len(feature_names))\n",
        "for i in range(len(feature_names)):\n",
        "    print(i,feature_names[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p35ehKV3Kq0z"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "    def __init__(self,filepath1,filepath2,verbose=True):\n",
        "        self.files = [filepath1,filepath2]\n",
        "        self.alignments=[]\n",
        "        self.labels=[]\n",
        "        self.verbose = verbose\n",
        "        self.max_lines = None\n",
        "        self.ties = 0\n",
        "        self.predictions = []\n",
        "        self.num_features = 1\n",
        "\n",
        "    def set_num_features(self,count):\n",
        "        self.num_features = count\n",
        "\n",
        "    def set_max_lines(self,lines):\n",
        "        '''Limit the dataset size to fit in RAM.'''\n",
        "        self.max_lines = lines\n",
        "        if self.verbose:\n",
        "            print('Maximum lines to load per file: %d'%lines)\n",
        "\n",
        "    def _count_lines_(self):\n",
        "        '''Show number of lines per input file.'''\n",
        "        count0 = 0\n",
        "        with gzip.open (self.files[0],'rt') as handle0:\n",
        "            for row in handle0:\n",
        "                count0 += 1\n",
        "        count1 = 0\n",
        "        with gzip.open(self.files[1],'rt') as handle1:\n",
        "            for row in handle1:\n",
        "                count1 += 1\n",
        "        minimum = min(count0,count1)\n",
        "        if self.verbose:\n",
        "            print('File0 size: %d %s'%(count0,self.files[0]))\n",
        "            print('File1 size: %d %s'%(count1,self.files[1]))\n",
        "        return minimum\n",
        "\n",
        "    def _load_line_(self,row):\n",
        "        '''Load data structure from one line of CSV file.'''\n",
        "        line = row.strip()\n",
        "        fields = line.split(',')\n",
        "        ints = [0] * self.num_features\n",
        "        # These fields come straight from the input file.\n",
        "        # These fields are grouped by which read they describe.\n",
        "        # P1 R1 = Parent 1, Read 1\n",
        "        ints[0] = int(fields[0]) # P1 R1 AS\n",
        "        ints[1] = int(fields[1]) # P1 R1 ED\n",
        "        P1R1_MAT = int(fields[2]) # P1 R1 MAT\n",
        "        ints[2] = int(fields[3]) # P1 R1 MM\n",
        "        ints[3] = int(fields[4]) # P1 R1 HQMM\n",
        "        ints[4] = int(fields[5]) # P1 R1 GO\n",
        "        ints[5] = int(fields[6]) # P1 R1 GE\n",
        "        ints[6] = int(fields[7]) # P1 R1 INS\n",
        "        ints[7] = int(fields[8]) # P1 R1 DELS\n",
        "        ints[8] = int(fields[9]) # P1 R1 HQINS\n",
        "        ints[9] = int(fields[10]) # P1 R1 HQDEL\n",
        "        #  = Parent 1, Read 2\n",
        "        ints[10] = int(fields[11]) # P1 R2 AS\n",
        "        ints[11] = int(fields[12]) # P1 R2 ED\n",
        "        P1R2_MAT = int(fields[13]) # P1 R2 MAT\n",
        "        ints[12] = int(fields[14]) # P1 R2 MM\n",
        "        ints[13] = int(fields[15]) # P1 R2 HQMM\n",
        "        ints[14] = int(fields[16]) # P1 R2 GO\n",
        "        ints[15] = int(fields[17]) # P1 R2 GE\n",
        "        ints[16] = int(fields[18]) # P1 R2 INS\n",
        "        ints[17] = int(fields[19]) # P1 R2 DELS\n",
        "        ints[18] = int(fields[20]) # P1 R2 HQINS\n",
        "        ints[19] = int(fields[21]) # P1 R2 HQDEL\n",
        "        # P2 R1 = Parent 2, Read 1\n",
        "        ints[20] = int(fields[22]) # P2 R1 AS\n",
        "        ints[21] = int(fields[23]) # P2 R1 ED\n",
        "        P2R1_MAT = int(fields[24]) # P2 R1 MAT\n",
        "        ints[22] = int(fields[25]) # P2 R1 MM\n",
        "        ints[23] = int(fields[26]) # P2 R1 HQMM\n",
        "        ints[24] = int(fields[27]) # P2 R1 GO\n",
        "        ints[25] = int(fields[28]) # P2 R1 GE\n",
        "        ints[26] = int(fields[29]) # P2 R1 INS\n",
        "        ints[27] = int(fields[30]) # P2 R1 DELS\n",
        "        ints[28] = int(fields[31]) # P2 R1 HQINS\n",
        "        ints[29] = int(fields[32]) # P2 R1 HQDEL\n",
        "        # P2 R2 = Parent 2, Read 2\n",
        "        ints[30] = int(fields[33]) # P2 R2 AS\n",
        "        ints[31] = int(fields[34]) # P2 R2 ED\n",
        "        P2R2_MAT = int(fields[35]) # P2 R2 MAT\n",
        "        ints[32] = int(fields[36]) # P2 R2 MM\n",
        "        ints[33] = int(fields[37]) # P2 R2 HQMM\n",
        "        ints[34] = int(fields[38]) # P2 R2 GO\n",
        "        ints[35] = int(fields[39]) # P2 R2 GE\n",
        "        ints[36] = int(fields[40]) # P2 R2 INS\n",
        "        ints[37] = int(fields[41]) # P2 R2 DELS\n",
        "        ints[38] = int(fields[42]) # P2 R2 HQINS\n",
        "        ints[39] = int(fields[43]) # P2 R2 HQDEL\n",
        "        # Fields that come in twos\n",
        "        R1_LEN = int(fields[44]) # R1 length (of read)\n",
        "        R2_LEN = int(fields[45]) # R2 length (of read)\n",
        "        P1_SPAN = int(fields[46]) # P1 span (of mapped read pair)\n",
        "        P2_SPAN = int(fields[47]) # P2 span (of mapped read pair)\n",
        "        # Read-wise differences\n",
        "        ints[40] = P2_SPAN-P1_SPAN # P2-P1 span diff\n",
        "        ints[41] = (ints[33]+ints[22])-(ints[11]+ints[0]) # AS diff\n",
        "        ints[42] = (ints[34]+ints[23])-(ints[12]+ints[1]) # ED diff\n",
        "        ints[43] = (P2R1_MAT+P2R2_MAT)-(P1R1_MAT+P1R2_MAT) # MAT diff\n",
        "        ints[44] = (ints[36]+ints[25])-(ints[14]+ints[3]) # MM diff\n",
        "        ints[45] = (ints[37]+ints[26])-(ints[15]+ints[4]) # HQMM diff\n",
        "        ints[46] = (ints[38]+ints[27])-(ints[16]+ints[5]) # GO diff\n",
        "        ints[47] = (ints[39]+ints[28])-(ints[17]+ints[6]) # GE diff\n",
        "        ints[48] = (ints[40]+ints[29])-(ints[18]+ints[7]) # INS diff\n",
        "        ints[49] = (ints[41]+ints[30])-(ints[19]+ints[8]) # DELS diff\n",
        "        ints[50] = (ints[42]+ints[31])-(ints[20]+ints[9]) # HQINS diff\n",
        "        ints[51] = (ints[43]+ints[32])-(ints[21]+ints[10]) # HQDEL diff\n",
        "        # The feature-extraction program populated a field\n",
        "        # to indicate which parent had higher alignment score.\n",
        "        # Values were 0=same, 1=parent1, 2=parent2.\n",
        "        # We change the values to -1=parent1, 0=unknown, +1=parent2\n",
        "        parent_choice = int(fields[48])\n",
        "        if parent_choice == 1:\n",
        "            ints[52] = -1  # not parent 2\n",
        "        elif parent_choice == 2:\n",
        "            ints[52] = 1  # is parent 2\n",
        "        elif parent_choice == 0:\n",
        "            ints[52] = 0\n",
        "        else:\n",
        "            raise Exception('Unrecognized parent choice:'+str(parent_choice))\n",
        "        # For fair comparison, force aligner to choose.\n",
        "        # We change 1 to 0, 2 to 1, and 0 to 1 or 2 randomly.\n",
        "        # TO DO: faster alternative to list.append() ???\n",
        "        parent_choice = int(fields[48])\n",
        "        if parent_choice == 1:\n",
        "            self.predictions.append(0)  # not parent 2\n",
        "        elif parent_choice == 2:\n",
        "            self.predictions.append(1)  # is parent 2\n",
        "        else: # parent_choice == 0:\n",
        "            self.ties += 1\n",
        "            guess = random.randint(0,1)\n",
        "            self.predictions.append(guess)\n",
        "        # The transcript that this read pair aligned to.\n",
        "        # This is for pipelines that only process reads that map\n",
        "        # to same transcript in both parents and (filter the others).\n",
        "        # Pipelines that assign reads to parent, regardless of which gene,\n",
        "        # should ignore this value. (It only reflects first parent map anyway.)\n",
        "        transcript_id = fields[49] # TO DO: where to put this?\n",
        "        self.alignments.append(ints)\n",
        "\n",
        "    def count_ties(self):\n",
        "        return self.ties\n",
        "\n",
        "    def load_full_train_set(self):\n",
        "        '''Load full train set (to be used for train and valiation).\n",
        "           Use set_max_lines() to leave some data for the test set.'''\n",
        "        minimum = 0\n",
        "        train_size = self.max_lines\n",
        "        if self.verbose:\n",
        "            print('Trying to load %d lines per file...'%train_size)\n",
        "        try:\n",
        "            handle0 = gzip.open(self.files[0],'rt')\n",
        "            handle1 = gzip.open(self.files[1],'rt')\n",
        "            # Associate label 0 with data from file 0. Same for 1.\n",
        "            for i in range(train_size):\n",
        "                row = next(handle0)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(0)\n",
        "                row = next(handle1)\n",
        "                self._load_line_(row)\n",
        "                self.labels.append(1)\n",
        "            handle0.close()\n",
        "            handle1.close()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print('Most likely, one file has too few reads.')\n",
        "            raise Exception('CANNOT LOAD DATA FROM FILE!')\n",
        "\n",
        "    def show_examples(self,head=6):\n",
        "        head = min(head,len(self.alignments))\n",
        "        for i in range(head):\n",
        "            print('From '+self.files[self.labels[i]])\n",
        "            print('Score,Edit,MM,HQMM,GapOpen,GapExtend,INS,DELS,HQINS,HQDEL')\n",
        "            print(self.alignments[i][0:9])\n",
        "            print(self.alignments[i][10:19])\n",
        "            print(self.alignments[i][20:29])\n",
        "            print(self.alignments[i][30:39])\n",
        "            print('Parent choice:',self.alignments[i][52])\n",
        "\n",
        "    def get_X_y(self):\n",
        "        loaded = len(self.alignments)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        X_train = np.array(self.alignments[:divider])\n",
        "        y_train = np.array(self.labels[:divider])\n",
        "        X_valid = np.array(self.alignments[divider:])\n",
        "        y_valid = np.array(self.labels[divider:])\n",
        "        if self.verbose:\n",
        "            print('Full train set size = '+str(len(self.alignments)))\n",
        "            print('Training/Validation partition: %d/%d'%(len(y_train),len(y_valid)))\n",
        "        return X_train,y_train, X_valid,y_valid\n",
        "\n",
        "    def get_predictions(self):\n",
        "        loaded = len(self.predictions)\n",
        "        divider = int(loaded - loaded * VALID_PORTION)\n",
        "        y_train = self.predictions[:divider]\n",
        "        y_valid = self.predictions[divider:]\n",
        "        return y_train, y_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pcZVyvS_126",
        "outputId": "199d6a85-a98c-460d-d517-700f3255577b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 15:48:01.142854\n",
            "Maximum lines to load per file: 1000000\n",
            "LOADING\n",
            "Trying to load 1000000 lines per file...\n",
            "Number of ties: 671711\n",
            "2023-11-12 15:48:49.777619\n"
          ]
        }
      ],
      "source": [
        "print(datetime.now())\n",
        "filepath0 = DATA_DIR+DATA_FILE_0\n",
        "filepath1 = DATA_DIR+DATA_FILE_1\n",
        "loader=DataLoader(filepath0,filepath1)\n",
        "loader.set_max_lines(MAX_LINES_TO_LOAD)\n",
        "loader.set_num_features(len(feature_names))\n",
        "print('LOADING')\n",
        "loader.load_full_train_set()\n",
        "print('Number of ties: %d' % loader.count_ties())\n",
        "aligner_predictions_train, aligner_predictions_valid = loader.get_predictions()\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7uHn9Ib_129",
        "outputId": "fc20519a-37f3-41ed-9652-40afed7f721b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full train set size = 2000000\n",
            "Training/Validation partition: 1600000/400000\n",
            "X train shape: \n",
            "(1600000, 53)\n",
            "y train shape: \n",
            "(1600000,)\n"
          ]
        }
      ],
      "source": [
        "X_train,y_train, X_valid,y_valid = loader.get_X_y()\n",
        "X_valid = None\n",
        "y_valid = None\n",
        "print('X train shape: ')\n",
        "print(np.shape(X_train))\n",
        "print('y train shape: ')\n",
        "print(np.shape(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDZ6siB_Kq04"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwMbRjm0FLBF"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    rfc = RFC()\n",
        "    return rfc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCzbj21FMpfU"
      },
      "outputs": [],
      "source": [
        "# docs: Note that in binary classification, recall of the positive class is also known as “sensitivity”; recall of the negative class is “specificity”.\n",
        "def show_performance(y_valid, yhat_classes, yhat_pred):\n",
        "    accuracy = accuracy_score(y_valid, yhat_classes)*100.\n",
        "    precision = precision_score(y_valid, yhat_classes)*100.\n",
        "    recall = recall_score(y_valid, yhat_classes)*100.\n",
        "    sensitivity = recall_score(y_valid, yhat_classes, pos_label=1)*100.\n",
        "    specificity = recall_score(y_valid, yhat_classes, pos_label=0)*100.\n",
        "    f1 = f1_score(y_valid, yhat_classes)*100.\n",
        "    mcc = matthews_corrcoef(y_valid, yhat_classes)\n",
        "    if yhat_pred is None:\n",
        "        # these stats are possible for probabilistic models only\n",
        "        auprc = 0.\n",
        "        auroc = 0.\n",
        "    else:\n",
        "        prc_Y, prc_X, prc_bins = precision_recall_curve(y_valid, yhat_pred)\n",
        "        auprc = auc(prc_X,prc_Y)*100.\n",
        "        auroc = roc_auc_score(y_valid, yhat_pred)*100.\n",
        "    values,counts=np.unique(yhat_classes, return_counts=True)\n",
        "    print('Predictions: ', dict(zip(values, counts)))\n",
        "    print('Accuracy: %.2f%% F1: %.2f%% MCC: %.4f' % (accuracy,f1,mcc))\n",
        "    print('Precision: %.2f%% Recall: %.2f%% AUPRC: %.2f%%' % (precision,recall,auprc))\n",
        "    print('Sensitivity: %.2f%% Specificity: %.2f%% AUROC: %.2f%%' % (sensitivity,specificity,auroc))\n",
        "    return accuracy,precision,recall,f1,sensitivity,specificity,mcc,auprc,auroc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfgplJ3Ep8Vr"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HCWG_w9_13F",
        "outputId": "78b959e0-b9c6-4a58-91f9-b0adf5e51a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-12 15:49:03.108417\n",
            "FOLD 1\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 339.45891523361206\n",
            "Distrib of scores: 0.5011463999039258 mean 0.39345065041251615 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[124322  35445]\n",
            " [ 15255 144978]]\n",
            "Normalized matrix\n",
            " [[0.38850625 0.11076563]\n",
            " [0.04767188 0.45305625]]\n",
            "Predictions:  {0: 139577, 1: 180423}\n",
            "Accuracy: 84.16% F1: 85.12% MCC: 0.6886\n",
            "Precision: 80.35% Recall: 90.48% AUPRC: 93.91%\n",
            "Sensitivity: 90.48% Specificity: 77.81% AUROC: 93.58%\n",
            "2023-11-12 15:55:04.409526\n",
            "FOLD 2\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 402.94403529167175\n",
            "Distrib of scores: 0.4988901931619389 mean 0.3930399538589355 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[124984  35154]\n",
            " [ 15337 144525]]\n",
            "Normalized matrix\n",
            " [[0.390575   0.10985625]\n",
            " [0.04792813 0.45164063]]\n",
            "Predictions:  {0: 140321, 1: 179679}\n",
            "Accuracy: 84.22% F1: 85.13% MCC: 0.6898\n",
            "Precision: 80.44% Recall: 90.41% AUPRC: 93.86%\n",
            "Sensitivity: 90.41% Specificity: 78.05% AUROC: 93.59%\n",
            "2023-11-12 16:02:10.222898\n",
            "FOLD 3\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 321.7582805156708\n",
            "Distrib of scores: 0.4998712577546222 mean 0.3930743251550901 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[124879  35244]\n",
            " [ 15407 144470]]\n",
            "Normalized matrix\n",
            " [[0.39024687 0.1101375 ]\n",
            " [0.04814687 0.45146875]]\n",
            "Predictions:  {0: 140286, 1: 179714}\n",
            "Accuracy: 84.17% F1: 85.08% MCC: 0.6888\n",
            "Precision: 80.39% Recall: 90.36% AUPRC: 93.89%\n",
            "Sensitivity: 90.36% Specificity: 77.99% AUROC: 93.60%\n",
            "2023-11-12 16:07:53.487749\n",
            "FOLD 4\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 506.0650577545166\n",
            "Distrib of scores: 0.4997092279310804 mean 0.3932866559821952 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[124960  35215]\n",
            " [ 15207 144618]]\n",
            "Normalized matrix\n",
            " [[0.3905     0.11004688]\n",
            " [0.04752187 0.45193125]]\n",
            "Predictions:  {0: 140167, 1: 179833}\n",
            "Accuracy: 84.24% F1: 85.16% MCC: 0.6903\n",
            "Precision: 80.42% Recall: 90.49% AUPRC: 93.92%\n",
            "Sensitivity: 90.49% Specificity: 78.01% AUROC: 93.64%\n",
            "2023-11-12 16:16:42.038687\n",
            "FOLD 5\n",
            "Training samples 1280000 Testing samples 320000\n",
            "Training...\n",
            "Elapsed seconds: 317.0744652748108\n",
            "Distrib of scores: 0.5004522440252016 mean 0.3931818224984712 std\n",
            "Range of scores: 0.0 to 1.0\n",
            "Confusion matrix\n",
            " [[124443  35354]\n",
            " [ 15118 145085]]\n",
            "Normalized matrix\n",
            " [[0.38888438 0.11048125]\n",
            " [0.04724375 0.45339062]]\n",
            "Predictions:  {0: 139561, 1: 180439}\n",
            "Accuracy: 84.23% F1: 85.18% MCC: 0.6900\n",
            "Precision: 80.41% Recall: 90.56% AUPRC: 93.86%\n",
            "Sensitivity: 90.56% Specificity: 77.88% AUROC: 93.57%\n",
            "2023-11-12 16:22:20.598930\n"
          ]
        }
      ],
      "source": [
        "print(datetime.now())\n",
        "FOLD = 0\n",
        "accuracyCV=list()\n",
        "precisionCV=list()\n",
        "recallCV=list()\n",
        "f1CV=list()\n",
        "sensitivityCV=list()\n",
        "specificityCV=list()\n",
        "mccCV=list()\n",
        "auprcCV=list()\n",
        "aurocCV=list()\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for train_index, test_index in kfold.split(X_train, y_train):\n",
        "    rfc_model=build_model()\n",
        "    FOLD += 1\n",
        "    print('FOLD', FOLD)\n",
        "    print('Training samples', len(train_index), 'Testing samples', len(test_index) )\n",
        "    print('Training...')\n",
        "    start_time = time.time()\n",
        "    rfc_model.fit(X_train[train_index], y_train[train_index])\n",
        "    end_time = time.time()\n",
        "    print('Elapsed seconds:', (end_time-start_time))\n",
        "\n",
        "    yhat_pairs=rfc_model.predict_proba(X_train[test_index])\n",
        "    yhat_pred=[pair[1] for pair in yhat_pairs]\n",
        "    yhat_classes=rfc_model.predict(X_train[test_index])\n",
        "\n",
        "    print('Distrib of scores:',np.mean(yhat_pred),'mean',np.std(yhat_pred),'std')\n",
        "    print('Range of scores:',np.min(yhat_pred),'to',np.max(yhat_pred))\n",
        "    cm1 = confusion_matrix(y_train[test_index],yhat_classes)\n",
        "    print('Confusion matrix\\n',cm1)\n",
        "    cm2 = confusion_matrix(y_train[test_index],yhat_classes,normalize='all')\n",
        "    print('Normalized matrix\\n',cm2)\n",
        "\n",
        "    accuracy,precision,recall,f1,sensitivity,specificity,mcc,auprc,auroc=\\\n",
        "        show_performance(y_train[test_index], yhat_classes, yhat_pred)\n",
        "    accuracyCV.append   (accuracy)\n",
        "    precisionCV.append  (precision)\n",
        "    recallCV.append     (recall)\n",
        "    f1CV.append         (f1)\n",
        "    sensitivityCV.append(sensitivity)\n",
        "    specificityCV.append(specificity)\n",
        "    mccCV.append        (mcc)\n",
        "    auprcCV.append      (auprc)\n",
        "    aurocCV.append      (auroc)\n",
        "    print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si8QbOpY_13G",
        "outputId": "a70c5b85-e0c5-4f7a-ccdc-31ea2b8dde3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy    84.20400000000001 0.033834038888374746\n",
            "precision   80.4006135455607 0.02753930373373326\n",
            "recall      90.45944946929322 0.06919743792566183\n",
            "f1          85.13392517524863 0.033500113199491366\n",
            "sensitivity 90.45944946929322 0.06919743792566183\n",
            "specificity 77.94840463824445 0.08846611198378972\n",
            "mcc         0.6894971570425967 0.0006975066549733766\n",
            "auprc       93.8876192283684 0.024592072359867302\n",
            "auroc       93.59346147801901 0.023150810734939863\n"
          ]
        }
      ],
      "source": [
        "print('accuracy   ',   np.mean(accuracyCV),    np.std(accuracyCV))\n",
        "print('precision  ',  np.mean(precisionCV),   np.std(precisionCV))\n",
        "print('recall     ',     np.mean(recallCV),      np.std(recallCV))\n",
        "print('f1         ',         np.mean(f1CV),          np.std(f1CV))\n",
        "print('sensitivity',np.mean(sensitivityCV), np.std(sensitivityCV))\n",
        "print('specificity',np.mean(specificityCV), np.std(specificityCV))\n",
        "print('mcc        ',        np.mean(mccCV),         np.std(mccCV))\n",
        "print('auprc      ',      np.mean(auprcCV),       np.std(auprcCV))\n",
        "print('auroc      ',      np.mean(aurocCV),       np.std(aurocCV))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}